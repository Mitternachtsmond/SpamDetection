{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Notebook - using SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add `archive.zip` (dataset) in your gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAAzQVjwYNFp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCHTpSU178eS"
      },
      "source": [
        "Unzipping `archive.zip` from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XXy6fpx0IjV4"
      },
      "outputs": [],
      "source": [
        "!unzip drive/My\\ Drive/archive.zip > /dev/null #-d drive/My\\ Drive/enron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "balxn2oo7w2o"
      },
      "source": [
        "### Combining all the datasets into one directory for final export of model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xk2K0CbhKeOw"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat << EOF > s.sh\n",
        "#!/bin/bash \n",
        "mkdir enron/{ham,spam} -p\n",
        "for i in \\$(ls | grep \"enron[1-6]\")\n",
        "do\n",
        "   cp \\$i/ham/* enron/ham\n",
        "   cp \\$i/spam/* enron/spam\n",
        "done\n",
        "EOF\n",
        "chmod +x s.sh\n",
        "./s.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6bb7QQIH8jR",
        "outputId": "1e75a8e2-23e5-411d-cb8f-2513860d6fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BWhILAv6Dj-B"
      },
      "outputs": [],
      "source": [
        "# Change the folder accordingly\n",
        "dir = \"enron1\" \n",
        "spam_list = [os.path.join(dir+\"/spam\",f) for f in os.listdir(dir+\"/spam\")]\n",
        "ham_list = [os.path.join(dir+\"/ham\",f) for f in os.listdir(dir+\"/ham\")]\n",
        "\n",
        "allHamData, allSpamData = [], []\n",
        "for obj in ham_list:\n",
        "  with open(obj,encoding='latin1') as ip:\n",
        "    allHamData.append(\" \".join(ip.readlines()))\n",
        "\n",
        "for obj in spam_list:\n",
        "  with open(obj,encoding='latin1') as ip:\n",
        "    allSpamData.append(\" \".join(ip.readlines()))\n",
        "\n",
        "allHamData = list(set(allHamData))\n",
        "allSpamData = list(set(allSpamData))\n",
        "\n",
        "hamPlusSpamData = allHamData + allSpamData\n",
        "labels = [\"ham\"]*len(allHamData) + [\"spam\"]*len(allSpamData)\n",
        "\n",
        "df = pd.DataFrame({\"email\": hamPlusSpamData, \"label\": labels})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBZHE-a6WynU",
        "outputId": "dc230c40-4714-4648-e6af-a6a0909a223a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[689   9]\n",
            " [  5 296]]\n",
            "SVM Accuracy:  98.5985985985986 %\n"
          ]
        }
      ],
      "source": [
        "cv_vec = sk.feature_extraction.text.TfidfVectorizer(tokenizer = nltk.word_tokenize, stop_words = nltk.corpus.stopwords.words(\"english\"))\n",
        "X = cv_vec.fit_transform(df.email)\n",
        "\n",
        "label_encoder = sk.preprocessing.LabelEncoder()\n",
        "y = label_encoder.fit_transform(df.label)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,  random_state=8, test_size=0.2)\n",
        "model = LinearSVC()\n",
        "model.fit(X_train,y_train)\n",
        "result = model.predict(X_test)\n",
        "print(confusion_matrix(y_test,result))\n",
        "print(\"SVM Accuracy: \",accuracy_score(y_test,result)*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv0QpPK4IQVO"
      },
      "outputs": [],
      "source": [
        "# SVM Accuracy of enron1:  98.7987987987988 %\n",
        "# SVM Accuracy of enron2:  99.14163090128756 %\n",
        "# SVM Accuracy of enron3:  99.0521327014218 %\n",
        "# SVM Accuracy of enron4:  98.80341880341881 %\n",
        "# SVM Accuracy of enron5:  99.21798631476051 %\n",
        "# SVM Accuracy of enron6:  98.41402337228715 %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44I3cL217U6F"
      },
      "source": [
        "## Combining all the datasets\n",
        "### ( preparing for export)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWEmT3PDc-3P"
      },
      "outputs": [],
      "source": [
        "cv_vec = sk.feature_extraction.text.TfidfVectorizer(tokenizer = nltk.word_tokenize, stop_words = nltk.corpus.stopwords.words(\"english\"))\n",
        "X = cv_vec.fit_transform(df.email)\n",
        "\n",
        "label_encoder = sk.preprocessing.LabelEncoder()\n",
        "y = label_encoder.fit_transform(df.label)\n",
        "\n",
        "model = LinearSVC()\n",
        "model.fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cstm93eFDzV",
        "outputId": "dad019d4-f2ef-465e-8cae-9af58fbe6512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cloudpickle==1.3.0\n",
            "pickleshare==0.7.5\n",
            "portpicker==1.3.9\n"
          ]
        }
      ],
      "source": [
        "!pip freeze | grep pic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE0nqic8Lymi"
      },
      "source": [
        "#### Save Model Using `pickle`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Csi-LzpP-4Ot"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(model, open('SVM.sav', 'wb'))\n",
        "pickle.dump(cv_vec,open('vectorizer.pk', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ntFOC7J_C-l"
      },
      "outputs": [],
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "fin = pickle.load(open('vectorizer.pk', 'rb'))\n",
        "text = '''Subject: re : 2 . 882 s - > np np\n",
        "> deat : sun , 15 dec 91 2 : 25 : 2 est > : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 query > > wlodek zadrozny ask \" anything interest \" > construction \" s > np np \" . . . second , > much relate : consider construction form > discuss list late reduplication ? > logical sense \" john mcnamara name \" tautologous thus , > level , indistinguishable \" , , here ? \" . ' john mcnamara name ' tautologous support those logic-base semantics irrelevant'''\n",
        "X = fin.transform([text])\n",
        "result = loaded_model.predict(X)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SVM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
