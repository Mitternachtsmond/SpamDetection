{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAAzQVjwYNFp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCHTpSU178eS"
      },
      "source": [
        "Unzipping `archive.zip` from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXy6fpx0IjV4"
      },
      "source": [
        "!unzip drive/My\\ Drive/archive.zip > /dev/null #-d drive/My\\ Drive/enron"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "balxn2oo7w2o"
      },
      "source": [
        "Combining all the datasets into one directory for final export of model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk2K0CbhKeOw"
      },
      "source": [
        "%%bash\n",
        "cat << EOF > s.sh\n",
        "#!/bin/bash \n",
        "mkdir enron/{ham,spam} -p\n",
        "for i in \\$(ls | grep \"enron[1-6]\")\n",
        "do\n",
        "   cp \\$i/ham/* enron/ham\n",
        "   cp \\$i/spam/* enron/spam\n",
        "done\n",
        "EOF\n",
        "chmod +x s.sh\n",
        "./s.sh"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6bb7QQIH8jR"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWhILAv6Dj-B"
      },
      "source": [
        "# Change the folder accordingly\n",
        "dir = \"enron\" \n",
        "spam_list = [os.path.join(dir+\"/spam\",f) for f in os.listdir(dir+\"/spam\")]\n",
        "ham_list = [os.path.join(dir+\"/ham\",f) for f in os.listdir(dir+\"/ham\")]\n",
        "\n",
        "allHamData, allSpamData = [], []\n",
        "for obj in ham_list:\n",
        "  with open(obj,encoding='latin1') as ip:\n",
        "    allHamData.append(\" \".join(ip.readlines()))\n",
        "\n",
        "for obj in spam_list:\n",
        "  with open(obj,encoding='latin1') as ip:\n",
        "    allSpamData.append(\" \".join(ip.readlines()))\n",
        "\n",
        "allHamData = list(set(allHamData))\n",
        "allSpamData = list(set(allSpamData))\n",
        "\n",
        "hamPlusSpamData = allHamData + allSpamData\n",
        "labels = [\"ham\"]*len(allHamData) + [\"spam\"]*len(allSpamData)\n",
        "\n",
        "df = pd.DataFrame({\"email\": hamPlusSpamData, \"label\": labels})"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBZHE-a6WynU"
      },
      "source": [
        "cv_vec = sk.feature_extraction.text.TfidfVectorizer(tokenizer = nltk.word_tokenize, stop_words = nltk.corpus.stopwords.words(\"english\"))\n",
        "X = cv_vec.fit_transform(df.email)\n",
        "\n",
        "label_encoder = sk.preprocessing.LabelEncoder()\n",
        "y = label_encoder.fit_transform(df.label)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,  random_state=8, test_size=0.2)\n",
        "model = LinearSVC()\n",
        "model.fit(X_train,y_train)\n",
        "result = model.predict(X_test)\n",
        "print(confusion_matrix(y_test,result))\n",
        "print(\"SVM Accuracy: \",accuracy_score(y_test,result)*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv0QpPK4IQVO"
      },
      "source": [
        "# SVM Accuracy of enron1:  98.7987987987988 %\n",
        "# SVM Accuracy of enron2:  99.14163090128756 %\n",
        "# SVM Accuracy of enron3:  99.0521327014218 %\n",
        "# SVM Accuracy of enron4:  98.80341880341881 %\n",
        "# SVM Accuracy of enron5:  99.21798631476051 %\n",
        "# SVM Accuracy of enron6:  98.41402337228715 %"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44I3cL217U6F"
      },
      "source": [
        "## Combining all the datasets\n",
        "###( preparing for export)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWEmT3PDc-3P"
      },
      "source": [
        "cv_vec = sk.feature_extraction.text.TfidfVectorizer(tokenizer = nltk.word_tokenize, stop_words = nltk.corpus.stopwords.words(\"english\"))\n",
        "X = cv_vec.fit_transform(df.email)\n",
        "\n",
        "label_encoder = sk.preprocessing.LabelEncoder()\n",
        "y = label_encoder.fit_transform(df.label)\n",
        "\n",
        "model = LinearSVC()\n",
        "model.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csi-LzpP-4Ot"
      },
      "source": [
        "# Save Model Using Pickle\n",
        "import pickle\n",
        "pickle.dump(model, open('SVM.sav', 'wb'))\n",
        "pickle.dump(cv_vec,open('vectorizer.pk', 'wb'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ntFOC7J_C-l"
      },
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "fin = pickle.load(open('vectorizer.pk', 'rb'))\n",
        "text = '''Subject: re : 2 . 882 s - > np np\n",
        "> deat : sun , 15 dec 91 2 : 25 : 2 est > : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 query > > wlodek zadrozny ask \" anything interest \" > construction \" s > np np \" . . . second , > much relate : consider construction form > discuss list late reduplication ? > logical sense \" john mcnamara name \" tautologous thus , > level , indistinguishable \" , , here ? \" . ' john mcnamara name ' tautologous support those logic-base semantics irrelevant'''\n",
        "X = fin.transform([text])\n",
        "result = loaded_model.predict(X)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}