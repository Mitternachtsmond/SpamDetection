{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_New.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAAzQVjwYNFp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXy6fpx0IjV4"
      },
      "source": [
        "!unzip drive/My\\ Drive/archive.zip > /dev/null #-d drive/My\\ Drive/enron"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6bb7QQIH8jR"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWhILAv6Dj-B"
      },
      "source": [
        "spam_dir = \"enron2/spam\" # spam folder\n",
        "ham_dir = \"enron2/ham\" # ham folder\n",
        "\n",
        "spam_list = [os.path.join(spam_dir,f) for f in os.listdir(spam_dir)]\n",
        "ham_list = [os.path.join(ham_dir,f) for f in os.listdir(ham_dir)]\n",
        "\n",
        "allHamData, allSpamData = [], []\n",
        "for obj in ham_list:\n",
        "  with open(obj,encoding='latin1') as ip:\n",
        "    allHamData.append(\" \".join(ip.readlines()))\n",
        "\n",
        "for obj in spam_list:\n",
        "  with open(obj,encoding='latin1') as ip:\n",
        "    allSpamData.append(\" \".join(ip.readlines()))\n",
        "\n",
        "allHamData = list(set(allHamData))\n",
        "allSpamData = list(set(allSpamData))\n",
        "\n",
        "hamPlusSpamData = allHamData + allSpamData\n",
        "labels = [\"ham\"]*len(allHamData) + [\"spam\"]*len(allSpamData)\n",
        "\n",
        "df = pd.DataFrame({\"email\": hamPlusSpamData, \"label\": labels})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBZHE-a6WynU"
      },
      "source": [
        "cv_vec = sk.feature_extraction.text.TfidfVectorizer(tokenizer = nltk.word_tokenize, stop_words = nltk.corpus.stopwords.words(\"english\"))\n",
        "X = cv_vec.fit_transform(df.email)\n",
        "\n",
        "label_encoder = sk.preprocessing.LabelEncoder()\n",
        "y = label_encoder.fit_transform(df.label)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,  random_state=8, test_size=0.2)\n",
        "model = LinearSVC()\n",
        "model.fit(X_train,y_train)\n",
        "result = model.predict(X_test)\n",
        "print(confusion_matrix(y_test,result))\n",
        "print(\"SVM Accuracy: \",accuracy_score(y_test,result)*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv0QpPK4IQVO"
      },
      "source": [
        "# SVM Accuracy of enron1:  98.7987987987988 %\n",
        "# SVM Accuracy of enron2:  99.14163090128756 %\n",
        "# SVM Accuracy of enron3:  99.0521327014218 %\n",
        "# SVM Accuracy of enron4:  98.80341880341881 %\n",
        "# SVM Accuracy of enron5:  99.21798631476051 %\n",
        "# SVM Accuracy of enron6:  98.41402337228715 %"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
